The main objective of gradient descent is to minimize the cost function or the error between expected and actual.

The cost function is defined as the measurement of difference or error between actual values and expected values at the current position and present in the form of a single real number.

Direction & Learning Rate - These two factors are used to determine the partial derivative calculation of future iteration and allow it to the point of convergence or local minimum or global minimum

LR - It is defined as the step size taken to reach the minimum or lowest point. small steps = to get precision

This code defines a basic gradient descent algorithm and visualizes its iterations on a given function

Iterations: The algorithm iterates until a certain number of iterations (n_iter) or until the change in the parameters falls below a specified tolerance (tolerance).

Gradient descent is a widely used optimization algorithm in machine learning for training models. It is employed to minimize the cost or loss function associated with a model by adjusting its parameters

Symbol: The Symbol function is used to create a symbolic variable. 
    This creates a symbolic variable x that can be used in symbolic expressions.
    
lambdify: The lambdify function is used to convert a symbolic expression into a callable function.
    It is often used when you have a symbolic expression representing a mathematical function, and 
    you want to generate a function that can be evaluated numerically.

the lambdify function to create a Python function (gradient) from the symbolic expression function.diff(x), 
    which represents the derivative of the function with respect to the variable x. The derivative is essentially 
    the gradient of the function. This gradient function will be used to 
    compute the gradient at each iteration of the gradient descent algorithm.
    
     the code is converting the original symbolic function (function) into 
        a Python function (function) using lambdify. This conversion is done to make it
        easier to evaluate the function at specific numerical values of x.



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sympy import Symbol, lambdify


x = Symbol('x')

 

def gradient_descent(
function, start, learn_rate, n_iter=10000, tolerance=1e-06, step_size=1
):
    
    gradient = lambdify(x, function.diff(x))
    function = lambdify(x, function)
    points = [start]
    iters = 0
    while step_size > tolerance and iters < n_iter:
        prev_x = start
        start = start - learn_rate * gradient(prev_x)
        step_size = abs(start - prev_x)
        iters = iters+1
        points.append(start)
    print("The local minimum occurs at", start)
    # Create plotting array
    x_ = np.linspace(-7,5,100)
    y = function(x_)
    # setting the axes at the centre
    fig = plt.figure(figsize = (10, 10))
    ax = fig.add_subplot(1, 1, 1)
    ax.spines['left'].set_position('center')
    ax.spines['bottom'].set_position('zero')
    ax.spines['right'].set_color('none')
    ax.spines['top'].set_color('none')
    ax.xaxis.set_ticks_position('bottom')
    ax.yaxis.set_ticks_position('left')
    # plot the function

    plt.plot(x_,y, 'r')
    plt.plot(points, function(np.array(points)), '-o')
    # show the plot
    plt.show()

function=(x+5)**2
gradient_descent(
    function=function, start=3.0, learn_rate=0.2, n_iter=50
)



